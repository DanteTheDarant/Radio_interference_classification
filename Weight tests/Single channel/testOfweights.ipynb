{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 08:07:15.207285: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-09 08:07:15.233788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers, callbacks\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "import pydot\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import pickle\n",
    "import dataframe_image as dfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data_amount = 20\n",
    "nr_classes = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotta load in some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../../All generated data/'\n",
    "labelpath = '../../../All generated labels/'\n",
    "data_list = os.listdir(datapath)\n",
    "#print(data_list)\n",
    "\n",
    "#all_data = [] #if we want to have data and labels in one list\n",
    "all_datapoints = []\n",
    "all_labels = []\n",
    "\n",
    "total_channels = 79\n",
    "total_scans_pr_sample = 20\n",
    "classes = 3\n",
    "\n",
    "\n",
    "for csv_file in data_list:\n",
    "    data_file = datapath + csv_file\n",
    "    current_data_file = pd.read_csv(data_file,header=None)\n",
    "\n",
    "    label_file = labelpath + csv_file\n",
    "    label_file = label_file.replace('.csv', '_labels.csv')\n",
    "    current_label_file = pd.read_csv(label_file,header=None)\n",
    "\n",
    "    for data_iter in range(len(current_data_file.index)):\n",
    "        #Pulling out the data from a row and putting it in the list\n",
    "        current_data_point = np.array(current_data_file.iloc[data_iter])\n",
    "        current_data_point = current_data_point.reshape(total_scans_pr_sample,total_channels)\n",
    "        all_datapoints.append(current_data_point)\n",
    "        \n",
    "        #adding the label to the datamatrix as the last row\n",
    "        label_row = np.array(current_label_file.iloc[data_iter])\n",
    "        label_row = label_row.reshape(1,total_channels)\n",
    "        all_labels.append(label_row)\n",
    "        \n",
    "        #all_data.append(np.vstack([current_data_point, label_row])) #if we want to have data and labels in one list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 79)\n",
      "10981\n",
      "10981\n"
     ]
    }
   ],
   "source": [
    "print(all_labels[1].shape)\n",
    "print(len(all_labels))\n",
    "print(len(all_datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick out one channel for each sample\n",
    "For now it takes the same channel for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "285506\n",
      "(1,)\n",
      "285506\n"
     ]
    }
   ],
   "source": [
    "chosen_channels = list(range(1,78,3))\n",
    "\n",
    "complete_data = []\n",
    "complete_labels = []\n",
    "\n",
    "for iter in range(len(all_datapoints)):\n",
    "    for channel in chosen_channels:\n",
    "        complete_data.append(all_datapoints[iter][0:time_data_amount,channel])\n",
    "        complete_labels.append(all_labels[iter][:,channel])\n",
    "    \n",
    "\n",
    "#quick check to make sure it works\n",
    "print(complete_data[1].shape)\n",
    "print(len(complete_data))\n",
    "print(complete_labels[1].shape)\n",
    "print(len(complete_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57102, 3)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(complete_data, complete_labels, train_size=0.8, random_state=112)\n",
    "\n",
    "# One hot encoding\n",
    "labels_test = to_categorical(labels_test)\n",
    "labels_train = to_categorical(labels_train)\n",
    "\n",
    "data_train = np.array(data_train)\n",
    "data_test = np.array(data_test)\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scaler from training data\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "\n",
    "# scale everything using that scaler\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff for class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
    "  \"\"\"\n",
    "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
    "  Some examples of different formats of class_series and their outputs are:\n",
    "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
    "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
    "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
    "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
    "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
    "  of appareance of the label when the dataset was processed. \n",
    "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
    "  Author: Angel Igareta (angel@igareta.com)\n",
    "  \"\"\"\n",
    "  if multi_class:\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "    if one_hot_encoded:\n",
    "      class_series = np.argmax(class_series, axis=1)\n",
    "  \n",
    "    # Compute class weights with sklearn method\n",
    "    class_labels = np.unique(class_series)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  else:\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\n",
    "    mlb = None\n",
    "    if not one_hot_encoded:\n",
    "      mlb = MultiLabelBinarizer()\n",
    "      class_series = mlb.fit_transform(class_series)\n",
    "\n",
    "    n_samples = len(class_series)\n",
    "    n_classes = len(class_series[0])\n",
    "\n",
    "    # Count each class frequency\n",
    "    class_count = [0] * n_classes\n",
    "    for classes in class_series:\n",
    "        for index in range(n_classes):\n",
    "            if classes[index] != 0:\n",
    "                class_count[index] += 1\n",
    "    \n",
    "    # Compute class weights using balanced method\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
    "    return dict(zip(class_labels, class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_list = []\n",
    "class_weights_list.append(np.array([1.0,1.0,1.0]))\n",
    "#class_weights_list.append(generate_class_weights(labels_train, multi_class=False, one_hot_encoded=True))\n",
    "\n",
    "for i in range(50):\n",
    "    _weights = np.random.default_rng().uniform(low=[0.25, 0.5, 0.5], high=[1, 1.5, 1.5], size=3)\n",
    "    class_weights_list.append(_weights)\n",
    "\n",
    "#print(class_weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"test_of_W\" directory already existed\n"
     ]
    }
   ],
   "source": [
    "def createDir(path: str):\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "        print('Created \"' + path + '\" directory')\n",
    "    else:\n",
    "        print('\"'+ path + '\" directory already existed')\n",
    "\n",
    "createDir('test_of_W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing all weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 08:07:27.501693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.505570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.505667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.507025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.507200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.507257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.928127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.928254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.928319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 08:07:27.928438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3429 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-05-09 08:07:29.120991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-09 08:07:29.912472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1785/1785 [==============================] - 2s 1ms/step - loss: 0.4159 - accuracy: 0.8567\n",
      "1785/1785 [==============================] - 1s 661us/step\n",
      "1785/1785 [==============================] - 2s 1ms/step - loss: 0.4717 - accuracy: 0.8337\n",
      "1785/1785 [==============================] - 1s 718us/step\n"
     ]
    }
   ],
   "source": [
    "idz = 0\n",
    "for weights_under_test in class_weights_list:\n",
    "    w_dict = dict(enumerate(weights_under_test, 0))\n",
    "\n",
    "    signal_size = time_data_amount\n",
    "\n",
    "    y = layers.Input(shape=(signal_size,1), dtype='float32', name='Input')\n",
    "\n",
    "    x = layers.Conv1D(16, 6, padding='same', activation='relu', use_bias=True)(y)\n",
    "    #x = layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "    x = layers.Conv1D(16, 3, padding='valid', activation='relu')(x)\n",
    "\n",
    "    '''\n",
    "    x = layers.MaxPool1D(pool_size=3,strides=1)(x)\n",
    "    #x = layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "    x = layers.Dropout(rate=0.1)(x)\n",
    "    x = layers.Conv1D(12, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Dropout(rate=0.1)(x)\n",
    "    x = layers.Conv1D(12, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPool1D(pool_size=3,strides=1)(x)\n",
    "    '''\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(rate=0.05)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(rate=0.2)(x)\n",
    "    x = layers.Dense(16,activation='relu')(x)\n",
    "    p = layers.Dense(classes, activation='softmax', name='p')(x)\n",
    "\n",
    "    model = Model(inputs=[y], outputs=[p])\n",
    "    #model.summary()\n",
    "\n",
    "    # ------------- model compilation --------------\n",
    "    ourAdam = Adam()\n",
    "    model.compile(optimizer=ourAdam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Set the model training parameters\n",
    "    # Stop model training when the training loss is not dropped\n",
    "    callbacks_list = [callbacks.EarlyStopping(\n",
    "                            monitor='val_loss', \n",
    "                            patience=15, \n",
    "                            verbose=0, \n",
    "                            mode='auto',\n",
    "                            restore_best_weights=True,\n",
    "                        )\n",
    "                                ]\n",
    "\n",
    "    # ------------- Starting model Training --------------\n",
    "    BATCH_SIZE = 4096\n",
    "    EPOCH = 200\n",
    "        \n",
    "\n",
    "    hist = model.fit(data_train,labels_train, \n",
    "            batch_size = BATCH_SIZE, \n",
    "            epochs = EPOCH,\n",
    "            verbose = 0,\n",
    "            callbacks= callbacks_list,\n",
    "            validation_split=0.25,\n",
    "            class_weight=w_dict)\n",
    "   \n",
    "    evalDict = model.evaluate(data_test,labels_test)\n",
    "\n",
    "    Y_test = np.argmax(labels_test, axis=1) # Convert one-hot to index\n",
    "    y_pred = np.argmax(model.predict(data_test),axis=1)\n",
    "    class_names = ['Empty channel', 'Wi-Fi', 'Bluetooth']\n",
    "    class_report = classification_report(Y_test, y_pred, target_names=class_names)\n",
    "\n",
    "    plt.figure()\n",
    "    ConfusionMatrixDisplay.from_predictions(Y_test, y_pred, normalize='true',cmap='Greens',colorbar=False, display_labels=class_names)\n",
    "    try:\n",
    "        plt.title(np.array2string(weights_under_test, precision=3, separator=','))\n",
    "    except Exception as gs:\n",
    "        print(gs)\n",
    "    plt.savefig('test_of_W/' + str(idz) + '_' + str(evalDict[1]) +'.pdf', format='pdf')\n",
    "    plt.close()\n",
    "    idz += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdb41d0cc79015bdcf6a93996e5168bcca6d7c1b72f3e6d100b9698b1014ae19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
