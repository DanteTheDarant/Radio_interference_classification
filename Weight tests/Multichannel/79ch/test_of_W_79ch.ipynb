{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 07:55:48.591629: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-09 07:55:48.618414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers, callbacks\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "import pydot\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import dataframe_image as dfi\n",
    "import pickle\n",
    "import math\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_width = 79\n",
    "time_data_amount = 20\n",
    "nr_classes = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotta load in some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../../All generated data/'\n",
    "labelpath = '../../../All generated labels/'\n",
    "data_list = os.listdir(datapath)\n",
    "#print(data_list)\n",
    "\n",
    "#all_data = [] #if we want to have data and labels in one list\n",
    "all_datapoints = []\n",
    "all_labels = []\n",
    "\n",
    "total_channels = 79\n",
    "total_scans_pr_sample = 20\n",
    "\n",
    "\n",
    "for csv_file in data_list:\n",
    "    data_file = datapath + csv_file\n",
    "    current_data_file = pd.read_csv(data_file,header=None)\n",
    "\n",
    "    label_file = labelpath + csv_file\n",
    "    label_file = label_file.replace('.csv', '_labels.csv')\n",
    "    current_label_file = pd.read_csv(label_file,header=None)\n",
    "\n",
    "    for data_iter in range(len(current_data_file.index)):\n",
    "        #Pulling out the data from a row and putting it in the list\n",
    "        current_data_point = np.array(current_data_file.iloc[data_iter])\n",
    "        current_data_point = current_data_point.reshape(total_scans_pr_sample,total_channels)\n",
    "        all_datapoints.append(current_data_point)\n",
    "        \n",
    "        #adding the label to the datamatrix as the last row\n",
    "        label_row = np.array(current_label_file.iloc[data_iter])\n",
    "        label_row = label_row.reshape(1,total_channels)\n",
    "        all_labels.append(label_row)\n",
    "        \n",
    "        #all_data.append(np.vstack([current_data_point, label_row])) #if we want to have data and labels in one list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 79)\n",
      "10981\n",
      "10981\n"
     ]
    }
   ],
   "source": [
    "print(all_labels[1].shape)\n",
    "print(len(all_labels))\n",
    "print(len(all_datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick out one channel for each sample\n",
    "For now it takes the same channel for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 79)\n",
      "10981\n",
      "(1, 79)\n",
      "10981\n"
     ]
    }
   ],
   "source": [
    "complete_data = []\n",
    "complete_labels = []\n",
    "\n",
    "\n",
    "for iter in range(len(all_datapoints)):\n",
    "    complete_data.append(all_datapoints[iter][0:time_data_amount,:])\n",
    "    complete_labels.append(all_labels[iter][:,:])\n",
    "    \n",
    "\n",
    "#quick check to make sure it works\n",
    "print(complete_data[1].shape)\n",
    "print(len(complete_data))\n",
    "print(complete_labels[1].shape)\n",
    "print(len(complete_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
    "  \"\"\"\n",
    "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
    "  Some examples of different formats of class_series and their outputs are:\n",
    "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
    "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
    "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
    "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
    "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
    "  of appareance of the label when the dataset was processed. \n",
    "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
    "  Author: Angel Igareta (angel@igareta.com)\n",
    "  \"\"\"\n",
    "  if multi_class:\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "    if one_hot_encoded:\n",
    "      class_series = np.argmax(class_series, axis=1)\n",
    "  \n",
    "    # Compute class weights with sklearn method\n",
    "    class_labels = np.unique(class_series)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  else:\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\n",
    "    mlb = None\n",
    "    if not one_hot_encoded:\n",
    "      mlb = preprocessing.MultiLabelBinarizer()\n",
    "      class_series = mlb.fit_transform(class_series)\n",
    "\n",
    "    n_samples = len(class_series)\n",
    "    n_classes = len(class_series[0])\n",
    "\n",
    "    # Count each class frequency\n",
    "    class_count = [0] * n_classes\n",
    "    for classes in class_series:\n",
    "        for index in range(n_classes):\n",
    "            if classes[index] != 0:\n",
    "                class_count[index] += 1\n",
    "    \n",
    "    # Compute class weights using balanced method\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  \n",
    "\n",
    "def weighted_mean_squared_error(class_weight):\n",
    "  def loss(y_true, y_pred):\n",
    "          y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "          y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "#             y_pred=  tf.transpose(y_pred, perm=[1, 0,2])\n",
    "\n",
    "          \n",
    "          weight = tf.constant(class_weight, dtype=tf.float32)\n",
    "          weight_per_sample = tf.transpose(tf.gather(weight, tf.argmax(y_true, axis=-1)))\n",
    "          weight_per_sample = tf.expand_dims(weight_per_sample, axis=-1)\n",
    "#             losses = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "          losses = tf.math.square(y_true-y_pred)*weight_per_sample\n",
    "          return tf.reduce_mean(losses, axis=-1)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2197, 1, 79)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(complete_data, complete_labels, train_size=0.8, random_state=112)\n",
    "\n",
    "# One hot encoding\n",
    "#labels_test = to_categorical(labels_test)\n",
    "#labels_train = to_categorical(labels_train)\n",
    "\n",
    "data_train = np.array(data_train)\n",
    "data_test = np.array(data_test)\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scaler from training data\n",
    "test = data_train[0]\n",
    "\n",
    "#reshape to 1d features\n",
    "nr_data_train = data_train.shape[0]\n",
    "data_train = data_train.reshape(nr_data_train, time_data_amount*channel_width)\n",
    "nr_data_test = data_test.shape[0]\n",
    "data_test = data_test.reshape(nr_data_test, time_data_amount*channel_width)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "\n",
    "# scale everything using that scaler\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "#reshaping back to 2d features\n",
    "data_train = data_train.reshape(nr_data_train, time_data_amount, channel_width)\n",
    "data_test = data_test.reshape(nr_data_test, time_data_amount, channel_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2197, 79)\n",
      "(8784, 79)\n",
      "(2197, 20, 79)\n"
     ]
    }
   ],
   "source": [
    "#fix shape\n",
    "labels_test = labels_test.reshape(nr_data_test,channel_width)\n",
    "labels_train = labels_train.reshape(nr_data_train,channel_width)\n",
    "data_train = data_train\n",
    "data_test = data_test\n",
    "print(labels_test.shape)\n",
    "print(labels_train.shape)\n",
    "print(data_test.shape)\n",
    "#print(labels_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2197, 79, 3)\n"
     ]
    }
   ],
   "source": [
    "labels_test = to_categorical(labels_test)\n",
    "labels_train = to_categorical(labels_train)\n",
    "print(labels_test.shape)\n",
    "#print(labels_test.reshape(920,3,79).shape)\n",
    "#print(labels_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding class weights \n",
    "Will only be used if weights is set to true in the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = generate_class_weights(labels_train.reshape([-1,3]), multi_class=True, one_hot_encoded=True)\n",
    "class_weights = np.array([class_weights[i] for i in range(len(class_weights))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"test_of_W\" directory already existed\n"
     ]
    }
   ],
   "source": [
    "def createDir(path: str):\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "        print('Created \"' + path + '\" directory')\n",
    "    else:\n",
    "        print('\"'+ path + '\" directory already existed')\n",
    "\n",
    "createDir('test_of_W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_list = []\n",
    "class_weights_list.append(np.array([1.0,1.0,1.0]))\n",
    "#class_weights_list.append(generate_class_weights(labels_train, multi_class=False, one_hot_encoded=True))\n",
    "\n",
    "for i in range(50):\n",
    "    _weights = np.random.default_rng().uniform(low=[0.25, 0.5, 0.5], high=[1, 1.5, 1.5], size=3)\n",
    "    class_weights_list.append(_weights)\n",
    "\n",
    "#print(class_weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 07:56:00.587922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.591158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.591266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.591981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.592078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.592134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.985080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.985216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.985282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:56:00.985347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3340 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-05-09 07:56:14.703122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-09 07:56:15.745677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 3s 42ms/step - loss: 3.0593 - out0_loss: 0.0903 - out1_loss: 0.0964 - out2_loss: 0.0857 - out3_loss: 0.0794 - out4_loss: 0.0626 - out5_loss: 0.0523 - out6_loss: 0.0562 - out7_loss: 0.0446 - out8_loss: 0.0404 - out9_loss: 0.0415 - out10_loss: 0.0381 - out11_loss: 0.0381 - out12_loss: 0.0335 - out13_loss: 0.0384 - out14_loss: 0.0419 - out15_loss: 0.0399 - out16_loss: 0.0465 - out17_loss: 0.0349 - out18_loss: 0.0335 - out19_loss: 0.0360 - out20_loss: 0.0340 - out21_loss: 0.0324 - out22_loss: 0.0379 - out23_loss: 0.0373 - out24_loss: 0.0398 - out25_loss: 0.0428 - out26_loss: 0.0410 - out27_loss: 0.0326 - out28_loss: 0.0388 - out29_loss: 0.0356 - out30_loss: 0.0323 - out31_loss: 0.0349 - out32_loss: 0.0362 - out33_loss: 0.0353 - out34_loss: 0.0336 - out35_loss: 0.0355 - out36_loss: 0.0402 - out37_loss: 0.0364 - out38_loss: 0.0402 - out39_loss: 0.0344 - out40_loss: 0.0346 - out41_loss: 0.0345 - out42_loss: 0.0374 - out43_loss: 0.0379 - out44_loss: 0.0358 - out45_loss: 0.0375 - out46_loss: 0.0452 - out47_loss: 0.0374 - out48_loss: 0.0337 - out49_loss: 0.0350 - out50_loss: 0.0317 - out51_loss: 0.0356 - out52_loss: 0.0339 - out53_loss: 0.0361 - out54_loss: 0.0328 - out55_loss: 0.0368 - out56_loss: 0.0368 - out57_loss: 0.0360 - out58_loss: 0.0360 - out59_loss: 0.0418 - out60_loss: 0.0341 - out61_loss: 0.0366 - out62_loss: 0.0369 - out63_loss: 0.0395 - out64_loss: 0.0420 - out65_loss: 0.0463 - out66_loss: 0.0432 - out67_loss: 0.0458 - out68_loss: 0.0467 - out69_loss: 0.0500 - out70_loss: 0.0471 - out71_loss: 0.0433 - out72_loss: 0.0446 - out73_loss: 0.0041 - out74_loss: 0.0040 - out75_loss: 0.0040 - out76_loss: 0.0036 - out77_loss: 3.1455e-08 - out78_loss: 7.1838e-08 - out0_accuracy: 0.7355 - out1_accuracy: 0.7132 - out2_accuracy: 0.7551 - out3_accuracy: 0.7547 - out4_accuracy: 0.8047 - out5_accuracy: 0.8666 - out6_accuracy: 0.8967 - out7_accuracy: 0.9153 - out8_accuracy: 0.9276 - out9_accuracy: 0.9326 - out10_accuracy: 0.9431 - out11_accuracy: 0.9290 - out12_accuracy: 0.9376 - out13_accuracy: 0.9317 - out14_accuracy: 0.9304 - out15_accuracy: 0.9331 - out16_accuracy: 0.9263 - out17_accuracy: 0.9208 - out18_accuracy: 0.9322 - out19_accuracy: 0.9422 - out20_accuracy: 0.9322 - out21_accuracy: 0.9322 - out22_accuracy: 0.9390 - out23_accuracy: 0.9390 - out24_accuracy: 0.9276 - out25_accuracy: 0.9135 - out26_accuracy: 0.9199 - out27_accuracy: 0.9231 - out28_accuracy: 0.9176 - out29_accuracy: 0.9345 - out30_accuracy: 0.9249 - out31_accuracy: 0.9117 - out32_accuracy: 0.9181 - out33_accuracy: 0.9335 - out34_accuracy: 0.9126 - out35_accuracy: 0.9099 - out36_accuracy: 0.9067 - out37_accuracy: 0.9290 - out38_accuracy: 0.8903 - out39_accuracy: 0.9190 - out40_accuracy: 0.9213 - out41_accuracy: 0.9240 - out42_accuracy: 0.8990 - out43_accuracy: 0.9122 - out44_accuracy: 0.9153 - out45_accuracy: 0.9053 - out46_accuracy: 0.9040 - out47_accuracy: 0.9235 - out48_accuracy: 0.9326 - out49_accuracy: 0.9326 - out50_accuracy: 0.9349 - out51_accuracy: 0.9404 - out52_accuracy: 0.9445 - out53_accuracy: 0.9399 - out54_accuracy: 0.9358 - out55_accuracy: 0.9263 - out56_accuracy: 0.9167 - out57_accuracy: 0.9285 - out58_accuracy: 0.9317 - out59_accuracy: 0.9304 - out60_accuracy: 0.9395 - out61_accuracy: 0.9244 - out62_accuracy: 0.9249 - out63_accuracy: 0.9281 - out64_accuracy: 0.9199 - out65_accuracy: 0.9030 - out66_accuracy: 0.9203 - out67_accuracy: 0.9144 - out68_accuracy: 0.9126 - out69_accuracy: 0.8839 - out70_accuracy: 0.8939 - out71_accuracy: 0.9254 - out72_accuracy: 0.9326 - out73_accuracy: 0.9909 - out74_accuracy: 0.9932 - out75_accuracy: 0.9909 - out76_accuracy: 0.9932 - out77_accuracy: 1.0000 - out78_accuracy: 1.0000\n",
      "69/69 [==============================] - 2s 18ms/step\n",
      "263.5696978569031\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 3.2441 - out0_loss: 0.0888 - out1_loss: 0.0913 - out2_loss: 0.0827 - out3_loss: 0.0759 - out4_loss: 0.0608 - out5_loss: 0.0537 - out6_loss: 0.0546 - out7_loss: 0.0459 - out8_loss: 0.0468 - out9_loss: 0.0444 - out10_loss: 0.0387 - out11_loss: 0.0412 - out12_loss: 0.0378 - out13_loss: 0.0414 - out14_loss: 0.0372 - out15_loss: 0.0429 - out16_loss: 0.0505 - out17_loss: 0.0371 - out18_loss: 0.0335 - out19_loss: 0.0365 - out20_loss: 0.0398 - out21_loss: 0.0371 - out22_loss: 0.0355 - out23_loss: 0.0407 - out24_loss: 0.0407 - out25_loss: 0.0433 - out26_loss: 0.0442 - out27_loss: 0.0367 - out28_loss: 0.0386 - out29_loss: 0.0384 - out30_loss: 0.0411 - out31_loss: 0.0370 - out32_loss: 0.0383 - out33_loss: 0.0441 - out34_loss: 0.0444 - out35_loss: 0.0395 - out36_loss: 0.0509 - out37_loss: 0.0363 - out38_loss: 0.0407 - out39_loss: 0.0449 - out40_loss: 0.0384 - out41_loss: 0.0357 - out42_loss: 0.0374 - out43_loss: 0.0393 - out44_loss: 0.0385 - out45_loss: 0.0403 - out46_loss: 0.0527 - out47_loss: 0.0413 - out48_loss: 0.0406 - out49_loss: 0.0377 - out50_loss: 0.0358 - out51_loss: 0.0377 - out52_loss: 0.0400 - out53_loss: 0.0372 - out54_loss: 0.0326 - out55_loss: 0.0384 - out56_loss: 0.0441 - out57_loss: 0.0381 - out58_loss: 0.0412 - out59_loss: 0.0410 - out60_loss: 0.0363 - out61_loss: 0.0392 - out62_loss: 0.0407 - out63_loss: 0.0443 - out64_loss: 0.0405 - out65_loss: 0.0472 - out66_loss: 0.0450 - out67_loss: 0.0417 - out68_loss: 0.0452 - out69_loss: 0.0544 - out70_loss: 0.0481 - out71_loss: 0.0506 - out72_loss: 0.0493 - out73_loss: 0.0050 - out74_loss: 0.0049 - out75_loss: 0.0043 - out76_loss: 0.0050 - out77_loss: 5.3562e-07 - out78_loss: 2.5407e-07 - out0_accuracy: 0.7251 - out1_accuracy: 0.7137 - out2_accuracy: 0.7674 - out3_accuracy: 0.7865 - out4_accuracy: 0.8261 - out5_accuracy: 0.8512 - out6_accuracy: 0.8817 - out7_accuracy: 0.9099 - out8_accuracy: 0.9222 - out9_accuracy: 0.9049 - out10_accuracy: 0.9426 - out11_accuracy: 0.9244 - out12_accuracy: 0.9267 - out13_accuracy: 0.9149 - out14_accuracy: 0.9126 - out15_accuracy: 0.9181 - out16_accuracy: 0.9122 - out17_accuracy: 0.9276 - out18_accuracy: 0.9208 - out19_accuracy: 0.9422 - out20_accuracy: 0.9294 - out21_accuracy: 0.9267 - out22_accuracy: 0.9194 - out23_accuracy: 0.9149 - out24_accuracy: 0.9108 - out25_accuracy: 0.8980 - out26_accuracy: 0.9026 - out27_accuracy: 0.9081 - out28_accuracy: 0.9035 - out29_accuracy: 0.8817 - out30_accuracy: 0.9226 - out31_accuracy: 0.9135 - out32_accuracy: 0.9181 - out33_accuracy: 0.8903 - out34_accuracy: 0.9176 - out35_accuracy: 0.9058 - out36_accuracy: 0.9112 - out37_accuracy: 0.9276 - out38_accuracy: 0.8990 - out39_accuracy: 0.8789 - out40_accuracy: 0.9008 - out41_accuracy: 0.9185 - out42_accuracy: 0.9144 - out43_accuracy: 0.9144 - out44_accuracy: 0.9172 - out45_accuracy: 0.9208 - out46_accuracy: 0.9112 - out47_accuracy: 0.9272 - out48_accuracy: 0.9103 - out49_accuracy: 0.9158 - out50_accuracy: 0.9299 - out51_accuracy: 0.9349 - out52_accuracy: 0.9263 - out53_accuracy: 0.9108 - out54_accuracy: 0.9199 - out55_accuracy: 0.9035 - out56_accuracy: 0.8962 - out57_accuracy: 0.8944 - out58_accuracy: 0.9222 - out59_accuracy: 0.9231 - out60_accuracy: 0.9222 - out61_accuracy: 0.9108 - out62_accuracy: 0.9167 - out63_accuracy: 0.9035 - out64_accuracy: 0.9076 - out65_accuracy: 0.8921 - out66_accuracy: 0.9149 - out67_accuracy: 0.9126 - out68_accuracy: 0.8876 - out69_accuracy: 0.8512 - out70_accuracy: 0.9090 - out71_accuracy: 0.8821 - out72_accuracy: 0.9094 - out73_accuracy: 0.9859 - out74_accuracy: 0.9850 - out75_accuracy: 0.9932 - out76_accuracy: 0.9904 - out77_accuracy: 1.0000 - out78_accuracy: 1.0000\n",
      "69/69 [==============================] - 2s 11ms/step\n",
      "188.94330668449402\n"
     ]
    }
   ],
   "source": [
    "idz = 0\n",
    "for weights_under_test in class_weights_list:\n",
    "    start = time()\n",
    "    w_dict = dict(enumerate(weights_under_test, 0))\n",
    "\n",
    "    signal_size = time_data_amount\n",
    "\n",
    "    y = layers.Input(shape=(signal_size,channel_width), dtype='float32', name='Input')\n",
    "    x = layers.Reshape((1,time_data_amount,channel_width), input_shape=(time_data_amount,channel_width))(y)\n",
    "\n",
    "    x = layers.Conv2D(32, [6,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Dropout(rate=0.1)(x)\n",
    "    x = layers.Conv2D(32, [2,5], padding='valid', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Dropout(rate=0.1)(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=1, padding='same', data_format='channels_first')(x)\n",
    "    x = layers.Conv2D(16, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Conv2D(16, [2,4], [1,2], padding='valid', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=1, padding='valid', data_format='channels_first')(x)\n",
    "    x = layers.Conv2D(8, [1,3], padding='valid', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Conv2D(8, [1,3], padding='valid', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "\n",
    "    class_layer = [{}]*channel_width\n",
    "    output_layer = [{}]*channel_width\n",
    "    for iter in range(channel_width):\n",
    "        class_layer[iter] = layers.Dropout(rate=0.25)(x)\n",
    "        class_layer[iter] = layers.Dense(32,activation='relu')(class_layer[iter])\n",
    "        #class_layer[iter] = layers.Dropout(rate=0.0)(class_layer[iter])\n",
    "        #class_layer[iter] = layers.Dense(16,activation='relu')(class_layer[iter])\n",
    "        output_layer[iter] = layers.Dense(nr_classes, activation='softmax', name=('out'+str(iter)))(class_layer[iter])\n",
    "\n",
    "    model = Model(inputs=[y], outputs=[out_layer for out_layer in output_layer])\n",
    "\n",
    "    # ------------- model compilation --------------\n",
    "    ourAdam = Adam()\n",
    "    optimizer = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "    loss_func = weighted_mean_squared_error(class_weights)\n",
    "\n",
    "    metric_T = 'accuracy'\n",
    "    loss_dict = {}\n",
    "    metric_dict = {}\n",
    "    for iter in range(channel_width):\n",
    "            loss_dict['out'+str(iter)] = loss_func\n",
    "            metric_dict['out'+str(iter)] = metric_T\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(optimizer=ourAdam, loss = loss_dict,\n",
    "              metrics=metric_T) \n",
    "                     \n",
    "\n",
    "    \n",
    "    # Set the model training parameters\n",
    "    # Stop model training when the training loss is not dropped\n",
    "    callbacks_list = [callbacks.EarlyStopping(\n",
    "                            monitor='val_loss', \n",
    "                            patience=5, \n",
    "                            verbose=0, \n",
    "                            mode='auto',\n",
    "                            restore_best_weights=True,\n",
    "                        )\n",
    "                                ]\n",
    "\n",
    "    # ------------- Starting model Training --------------\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCH = 50\n",
    "        \n",
    "\n",
    "    hist = model.fit(data_train,[labels_train[:,iter,:] for iter in range(channel_width)], \n",
    "            batch_size = BATCH_SIZE, \n",
    "            epochs = EPOCH, \n",
    "            callbacks= callbacks_list,\n",
    "            verbose = 0,\n",
    "            validation_split=0.25)\n",
    "       \n",
    "    evalDict = model.evaluate(data_test,[labels_test[:,iter,:] for iter in range(channel_width)])\n",
    "    \n",
    "    #Test on test data\n",
    "    true_test_labels = np.argmax(labels_test, axis=-1)\n",
    "    test_predictions = model.predict(data_test)\n",
    "    test_result = np.argmax(test_predictions, axis=-1).T\n",
    "    class_names = ['Empty channel', 'Wi-Fi', 'Bluetooth']\n",
    "    totalA = 0\n",
    "    \n",
    "    for i in range(total_channels+1,total_channels+1+total_channels):\n",
    "        totalA += evalDict[i]\n",
    "\n",
    "    totalA /= total_channels\n",
    "\n",
    "    plt.figure()\n",
    "    ConfusionMatrixDisplay.from_predictions(true_test_labels.flatten(), test_result.flatten(),normalize='true',cmap='Greens',colorbar=False,display_labels=class_names)\n",
    "    try:\n",
    "        plt.title(np.array2string(weights_under_test, precision=3, separator=','))\n",
    "    except Exception as gs:\n",
    "        print(gs)\n",
    "    plt.savefig('test_of_W/' + str(idz) + '_' + str(totalA) +'.pdf', format='pdf')\n",
    "    plt.close()\n",
    "    idz += 1\n",
    "    stop = time()\n",
    "    print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdb41d0cc79015bdcf6a93996e5168bcca6d7c1b72f3e6d100b9698b1014ae19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
