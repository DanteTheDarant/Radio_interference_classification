{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 07:43:11.719816: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-09 07:43:11.872969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers, callbacks\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "import pydot\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import dataframe_image as dfi\n",
    "import pickle\n",
    "import math\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_width = 5\n",
    "time_data_amount = 20\n",
    "nr_classes = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotta load in some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../../All generated data/'\n",
    "labelpath = '../../../All generated labels/'\n",
    "data_list = os.listdir(datapath)\n",
    "#print(data_list)\n",
    "\n",
    "#all_data = [] #if we want to have data and labels in one list\n",
    "all_datapoints = []\n",
    "all_labels = []\n",
    "\n",
    "total_channels = 79\n",
    "total_scans_pr_sample = 20\n",
    "\n",
    "\n",
    "for csv_file in data_list:\n",
    "    data_file = datapath + csv_file\n",
    "    current_data_file = pd.read_csv(data_file,header=None)\n",
    "\n",
    "    label_file = labelpath + csv_file\n",
    "    label_file = label_file.replace('.csv', '_labels.csv')\n",
    "    current_label_file = pd.read_csv(label_file,header=None)\n",
    "\n",
    "    for data_iter in range(len(current_data_file.index)):\n",
    "        #Pulling out the data from a row and putting it in the list\n",
    "        current_data_point = np.array(current_data_file.iloc[data_iter])\n",
    "        current_data_point = current_data_point.reshape(total_scans_pr_sample,total_channels)\n",
    "        all_datapoints.append(current_data_point)\n",
    "        \n",
    "        #adding the label to the datamatrix as the last row\n",
    "        label_row = np.array(current_label_file.iloc[data_iter])\n",
    "        label_row = label_row.reshape(1,total_channels)\n",
    "        all_labels.append(label_row)\n",
    "        \n",
    "        #all_data.append(np.vstack([current_data_point, label_row])) #if we want to have data and labels in one list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 79)\n",
      "10981\n",
      "10981\n"
     ]
    }
   ],
   "source": [
    "print(all_labels[1].shape)\n",
    "print(len(all_labels))\n",
    "print(len(all_datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick out one channel for each sample\n",
    "For now it takes the same channel for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 5)\n",
      "131772\n",
      "(1, 5)\n",
      "131772\n"
     ]
    }
   ],
   "source": [
    "chosen_channels = list(range(3,75,6))\n",
    "\n",
    "complete_data = []\n",
    "complete_labels = []\n",
    "\n",
    "lower_channel = math.floor(channel_width/2)\n",
    "upper_channel = math.ceil(channel_width/2)\n",
    "\n",
    "# check if channels are viable\n",
    "for channel in chosen_channels:\n",
    "        if (channel - lower_channel) < 0 or (channel + upper_channel - 1) > total_channels:\n",
    "            print('Bad channel choice')\n",
    "            exit()\n",
    "        if channel_width % 2 == 0:\n",
    "            print('please pick uneven channel width')\n",
    "            exit()\n",
    "\n",
    "\n",
    "for iter in range(len(all_datapoints)):\n",
    "    for channel in chosen_channels:\n",
    "        complete_data.append(all_datapoints[iter][0:time_data_amount,channel-lower_channel:channel+upper_channel])\n",
    "        complete_labels.append(all_labels[iter][:,channel-lower_channel:channel+upper_channel])\n",
    "    \n",
    "\n",
    "#quick check to make sure it works\n",
    "print(complete_data[1].shape)\n",
    "print(len(complete_data))\n",
    "print(complete_labels[1].shape)\n",
    "print(len(complete_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
    "  \"\"\"\n",
    "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
    "  Some examples of different formats of class_series and their outputs are:\n",
    "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
    "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
    "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
    "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
    "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
    "  of appareance of the label when the dataset was processed. \n",
    "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
    "  Author: Angel Igareta (angel@igareta.com)\n",
    "  \"\"\"\n",
    "  if multi_class:\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "    if one_hot_encoded:\n",
    "      class_series = np.argmax(class_series, axis=1)\n",
    "  \n",
    "    # Compute class weights with sklearn method\n",
    "    class_labels = np.unique(class_series)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  else:\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\n",
    "    mlb = None\n",
    "    if not one_hot_encoded:\n",
    "      mlb = preprocessing.MultiLabelBinarizer()\n",
    "      class_series = mlb.fit_transform(class_series)\n",
    "\n",
    "    n_samples = len(class_series)\n",
    "    n_classes = len(class_series[0])\n",
    "\n",
    "    # Count each class frequency\n",
    "    class_count = [0] * n_classes\n",
    "    for classes in class_series:\n",
    "        for index in range(n_classes):\n",
    "            if classes[index] != 0:\n",
    "                class_count[index] += 1\n",
    "    \n",
    "    # Compute class weights using balanced method\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  \n",
    "\n",
    "def weighted_mean_squared_error(class_weight):\n",
    "  def loss(y_true, y_pred):\n",
    "          y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "          y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "#             y_pred=  tf.transpose(y_pred, perm=[1, 0,2])\n",
    "\n",
    "          \n",
    "          weight = tf.constant(class_weight, dtype=tf.float32)\n",
    "          weight_per_sample = tf.transpose(tf.gather(weight, tf.argmax(y_true, axis=-1)))\n",
    "          weight_per_sample = tf.expand_dims(weight_per_sample, axis=-1)\n",
    "#             losses = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "          losses = tf.math.square(y_true-y_pred)*weight_per_sample\n",
    "          return tf.reduce_mean(losses, axis=-1)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26355, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(complete_data, complete_labels, train_size=0.8, random_state=112)\n",
    "\n",
    "# One hot encoding\n",
    "#labels_test = to_categorical(labels_test)\n",
    "#labels_train = to_categorical(labels_train)\n",
    "\n",
    "data_train = np.array(data_train)\n",
    "data_test = np.array(data_test)\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scaler from training data\n",
    "test = data_train[0]\n",
    "\n",
    "#reshape to 1d features\n",
    "nr_data_train = data_train.shape[0]\n",
    "data_train = data_train.reshape(nr_data_train, time_data_amount*channel_width)\n",
    "nr_data_test = data_test.shape[0]\n",
    "data_test = data_test.reshape(nr_data_test, time_data_amount*channel_width)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "\n",
    "# scale everything using that scaler\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "#reshaping back to 2d features\n",
    "data_train = data_train.reshape(nr_data_train, time_data_amount, channel_width)\n",
    "data_test = data_test.reshape(nr_data_test, time_data_amount, channel_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26355, 5)\n",
      "(105417, 5)\n",
      "(26355, 20, 5)\n"
     ]
    }
   ],
   "source": [
    "#fix shape\n",
    "labels_test = labels_test.reshape(nr_data_test,channel_width)\n",
    "labels_train = labels_train.reshape(nr_data_train,channel_width)\n",
    "data_train = data_train\n",
    "data_test = data_test\n",
    "print(labels_test.shape)\n",
    "print(labels_train.shape)\n",
    "print(data_test.shape)\n",
    "#print(labels_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26355, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "labels_test = to_categorical(labels_test)\n",
    "labels_train = to_categorical(labels_train)\n",
    "print(labels_test.shape)\n",
    "#print(labels_test.reshape(920,3,79).shape)\n",
    "#print(labels_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding class weights \n",
    "Will only be used if weights is set to true in the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = generate_class_weights(labels_train.reshape([-1,3]), multi_class=True, one_hot_encoded=True)\n",
    "class_weights = np.array([class_weights[i] for i in range(len(class_weights))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"test_of_W_3\" directory already existed\n"
     ]
    }
   ],
   "source": [
    "def createDir(path: str):\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "        print('Created \"' + path + '\" directory')\n",
    "    else:\n",
    "        print('\"'+ path + '\" directory already existed')\n",
    "\n",
    "createDir('test_of_W_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_list = []\n",
    "class_weights_list.append(np.array([1,1,1]))\n",
    "#class_weights_list.append(generate_class_weights(labels_train, multi_class=False, one_hot_encoded=True))\n",
    "\n",
    "for i in range(50):\n",
    "    _weights = np.random.default_rng().uniform(low=[0.25, 0.5, 0.5], high=[1, 1.5, 1.5], size=3)\n",
    "    class_weights_list.append(_weights)\n",
    "\n",
    "#print(class_weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 07:43:25.438349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:25.452518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:25.452625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:25.453398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:25.453494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:25.453555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:26.101332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:26.101461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:26.101523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 07:43:26.101608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3555 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-05-09 07:43:28.471765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-09 07:43:29.747256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824/824 [==============================] - 4s 4ms/step - loss: 0.2347 - out0_loss: 0.0488 - out1_loss: 0.0486 - out2_loss: 0.0401 - out3_loss: 0.0470 - out4_loss: 0.0501 - out0_accuracy: 0.9267 - out1_accuracy: 0.9266 - out2_accuracy: 0.9389 - out3_accuracy: 0.9295 - out4_accuracy: 0.9262\n",
      "824/824 [==============================] - 3s 3ms/step\n",
      "262.34864830970764\n",
      "824/824 [==============================] - 3s 4ms/step - loss: 0.2712 - out0_loss: 0.0596 - out1_loss: 0.0575 - out2_loss: 0.0461 - out3_loss: 0.0524 - out4_loss: 0.0555 - out0_accuracy: 0.9073 - out1_accuracy: 0.9185 - out2_accuracy: 0.9310 - out3_accuracy: 0.9252 - out4_accuracy: 0.9170\n",
      "824/824 [==============================] - 2s 2ms/step\n",
      "138.911847114563\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 71\u001b[0m\n\u001b[1;32m     67\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m     68\u001b[0m EPOCH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m---> 71\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchannel_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m evalDict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(data_test,[labels_test[:,\u001b[38;5;28miter\u001b[39m,:] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(channel_width)])\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#Test on test data\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idz = 0\n",
    "for weights_under_test in class_weights_list:\n",
    "    start = time()\n",
    "    w_dict = dict(enumerate(weights_under_test, 0))\n",
    "\n",
    "    signal_size = time_data_amount\n",
    "\n",
    "    y = layers.Input(shape=(signal_size,channel_width), dtype='float32', name='Input')\n",
    "    x = layers.Reshape((1,time_data_amount,channel_width), input_shape=(time_data_amount,channel_width))(y)\n",
    "\n",
    "    x = layers.Conv2D(32, [6,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Dropout(rate=0.1)(x)\n",
    "    x = layers.Conv2D(32, [2,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Dropout(rate=0.1)(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=1, padding='same', data_format='channels_first')(x)\n",
    "    x = layers.Conv2D(16, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Conv2D(16, [2,3], [1,2], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=1, padding='valid', data_format='channels_first')(x)\n",
    "    x = layers.Conv2D(16, [1,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Conv2D(16, [1,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "\n",
    "    class_layer = [{}]*channel_width\n",
    "    output_layer = [{}]*channel_width\n",
    "    for iter in range(channel_width):\n",
    "        class_layer[iter] = layers.Dropout(rate=0.25)(x)\n",
    "        class_layer[iter] = layers.Dense(32,activation='relu')(class_layer[iter])\n",
    "        #class_layer[iter] = layers.Dropout(rate=0.0)(class_layer[iter])\n",
    "        #class_layer[iter] = layers.Dense(16,activation='relu')(class_layer[iter])\n",
    "        output_layer[iter] = layers.Dense(nr_classes, activation='softmax', name=('out'+str(iter)))(class_layer[iter])\n",
    "\n",
    "    model = Model(inputs=[y], outputs=[out_layer for out_layer in output_layer])\n",
    "\n",
    "    # ------------- model compilation --------------\n",
    "    ourAdam = Adam()\n",
    "    optimizer = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "    loss_func = weighted_mean_squared_error(class_weights)\n",
    "\n",
    "    metric_T = 'accuracy'\n",
    "    loss_dict = {}\n",
    "    metric_dict = {}\n",
    "    for iter in range(channel_width):\n",
    "            loss_dict['out'+str(iter)] = loss_func\n",
    "            metric_dict['out'+str(iter)] = metric_T\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(optimizer=ourAdam, loss = loss_dict,\n",
    "              metrics=metric_T) \n",
    "                     \n",
    "\n",
    "    \n",
    "    # Set the model training parameters\n",
    "    # Stop model training when the training loss is not dropped\n",
    "    callbacks_list = [callbacks.EarlyStopping(\n",
    "                            monitor='val_loss', \n",
    "                            patience=10, \n",
    "                            verbose=0, \n",
    "                            mode='auto',\n",
    "                            restore_best_weights=True,\n",
    "                        )\n",
    "                                ]\n",
    "\n",
    "    # ------------- Starting model Training --------------\n",
    "    BATCH_SIZE = 1024\n",
    "    EPOCH = 200\n",
    "        \n",
    "\n",
    "    hist = model.fit(data_train,[labels_train[:,iter,:] for iter in range(channel_width)], \n",
    "            batch_size = BATCH_SIZE, \n",
    "            epochs = EPOCH, \n",
    "            callbacks= callbacks_list,\n",
    "            verbose = 0,\n",
    "            validation_split=0.25)\n",
    "       \n",
    "    evalDict = model.evaluate(data_test,[labels_test[:,iter,:] for iter in range(channel_width)])\n",
    "    \n",
    "    #Test on test data\n",
    "    true_test_labels = np.argmax(labels_test, axis=-1)\n",
    "    test_predictions = model.predict(data_test)\n",
    "    test_result = np.argmax(test_predictions, axis=-1).T\n",
    "    class_names = ['Empty channel', 'Wi-Fi', 'Bluetooth']\n",
    "    totalA = 0\n",
    "    \n",
    "    for i in range(channel_width+1,channel_width+1+channel_width):\n",
    "        totalA += evalDict[i]\n",
    "\n",
    "    totalA /= channel_width\n",
    "\n",
    "    plt.figure()\n",
    "    ConfusionMatrixDisplay.from_predictions(true_test_labels.flatten(), test_result.flatten(),normalize='true',cmap='Greens',colorbar=False,display_labels=class_names)\n",
    "    try:\n",
    "        plt.title(np.array2string(weights_under_test, precision=3, separator=','))\n",
    "    except Exception as gs:\n",
    "        print(gs)\n",
    "    plt.savefig('test_of_W_3/' + str(idz) + '_' + str(totalA) +'.pdf', format='pdf')\n",
    "    plt.close()\n",
    "    idz += 1\n",
    "    stop = time()\n",
    "    print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_size = time_data_amount\n",
    "\n",
    "y = layers.Input(shape=(signal_size,channel_width), dtype='float32', name='Input')\n",
    "x = layers.Reshape((1,time_data_amount,channel_width), input_shape=(time_data_amount,channel_width))(y)\n",
    "\n",
    "x = layers.Conv2D(32, [6,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "x = layers.Dropout(rate=0.1)(x)\n",
    "x = layers.Conv2D(32, [2,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "x = layers.Dropout(rate=0.1)(x)\n",
    "x = layers.MaxPool2D(pool_size=3, strides=1, padding='same', data_format='channels_first')(x)\n",
    "x = layers.Conv2D(16, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "x = layers.Conv2D(16, [2,3], [1,2], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "x = layers.MaxPool2D(pool_size=3, strides=1, padding='valid', data_format='channels_first')(x)\n",
    "x = layers.Conv2D(16, [1,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "x = layers.Conv2D(16, [1,3], padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "\n",
    "class_layer = [{}]*channel_width\n",
    "output_layer = [{}]*channel_width\n",
    "for iter in range(channel_width):\n",
    "    class_layer[iter] = layers.Dropout(rate=0.25)(x)\n",
    "    class_layer[iter] = layers.Dense(32,activation='relu')(class_layer[iter])\n",
    "    #class_layer[iter] = layers.Dropout(rate=0.0)(class_layer[iter])\n",
    "    #class_layer[iter] = layers.Dense(16,activation='relu')(class_layer[iter])\n",
    "    output_layer[iter] = layers.Dense(nr_classes, activation='softmax', name=('out'+str(iter)))(class_layer[iter])\n",
    "\n",
    "model = Model(inputs=[y], outputs=[out_layer for out_layer in output_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.8.10 (default, Mar 13 2023, 10:26:41) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdb41d0cc79015bdcf6a93996e5168bcca6d7c1b72f3e6d100b9698b1014ae19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
