{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers, callbacks\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "import pydot\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import dataframe_image as dfi\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from keras.utils.layer_utils import count_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_widths = [3, 5, 7, 9, 15, 21] \n",
    "sfs = [4,6, 8, 12, 16, 32]\n",
    "time_data_amounts = [8, 12, 16, 20]\n",
    "nr_classes = 3\n",
    "using_weights = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotta load in some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../../All generated data/'\n",
    "labelpath = '../../../All generated labels/'\n",
    "data_list = os.listdir(datapath)\n",
    "#print(data_list)\n",
    "\n",
    "#all_data = [] #if we want to have data and labels in one list\n",
    "all_datapoints = []\n",
    "all_labels = []\n",
    "\n",
    "total_channels = 79\n",
    "total_scans_pr_sample = 20\n",
    "\n",
    "\n",
    "for csv_file in data_list:\n",
    "    data_file = datapath + csv_file\n",
    "    current_data_file = pd.read_csv(data_file,header=None)\n",
    "\n",
    "    label_file = labelpath + csv_file\n",
    "    label_file = label_file.replace('.csv', '_labels.csv')\n",
    "    current_label_file = pd.read_csv(label_file,header=None)\n",
    "\n",
    "    for data_iter in range(len(current_data_file.index)):\n",
    "        #Pulling out the data from a row and putting it in the list\n",
    "        current_data_point = np.array(current_data_file.iloc[data_iter])\n",
    "        current_data_point = current_data_point.reshape(total_scans_pr_sample,total_channels)\n",
    "        all_datapoints.append(current_data_point)\n",
    "        \n",
    "        #adding the label to the datamatrix as the last row\n",
    "        label_row = np.array(current_label_file.iloc[data_iter])\n",
    "        label_row = label_row.reshape(1,total_channels)\n",
    "        all_labels.append(label_row)\n",
    "        \n",
    "        #all_data.append(np.vstack([current_data_point, label_row])) #if we want to have data and labels in one list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_labels[1].shape)\n",
    "print(len(all_labels))\n",
    "print(len(all_datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick out one channel for each sample\n",
    "For now it takes the same channel for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_data_amount in time_data_amounts:\n",
    "    for channel_width in channel_widths:\n",
    "        for sf in sfs:\n",
    "            name = 'multi_channels_test' + str(channel_width) #name of model - should be descriptive\n",
    "            save_folder = 'sf'+ str(sf) + '_' + str(channel_width) + 'Ch' + str(time_data_amount) + '_VGG' #hyperparameter description her\n",
    "            if using_weights == True:\n",
    "                save_folder = save_folder + '_W'\n",
    "                name = name + '_weighted'\n",
    "\n",
    "            complete_data = []\n",
    "            complete_labels = []\n",
    "\n",
    "            lower_channel = math.floor(channel_width/2)\n",
    "            upper_channel = math.ceil(channel_width/2)\n",
    "\n",
    "            chosen_channels = list(range(upper_channel+1,79-upper_channel,channel_width))\n",
    "\n",
    "            # check if channels are viable\n",
    "            for channel in chosen_channels:\n",
    "                    if (channel - lower_channel) < 0 or (channel + upper_channel - 1) > total_channels:\n",
    "                        print('Bad channel choice')\n",
    "                        exit()\n",
    "                    if channel_width % 2 == 0:\n",
    "                        print('please pick uneven channel width')\n",
    "                        exit()\n",
    "\n",
    "\n",
    "            for iter in range(len(all_datapoints)):\n",
    "                for channel in chosen_channels:\n",
    "                    complete_data.append(all_datapoints[iter][0:time_data_amount,channel-lower_channel:channel+upper_channel])\n",
    "                    complete_labels.append(all_labels[iter][:,channel-lower_channel:channel+upper_channel])\n",
    "\n",
    "\n",
    "            data_train, data_test, labels_train, labels_test = train_test_split(complete_data, complete_labels, train_size=0.8, random_state=112)\n",
    "\n",
    "            data_train = np.array(data_train)\n",
    "            data_test = np.array(data_test)\n",
    "            labels_train = np.array(labels_train)\n",
    "            labels_test = np.array(labels_test)\n",
    "\n",
    "\n",
    "            #reshape to 1d features\n",
    "            nr_data_train = data_train.shape[0]\n",
    "            data_train = data_train.reshape(nr_data_train, time_data_amount*channel_width)\n",
    "            nr_data_test = data_test.shape[0]\n",
    "            data_test = data_test.reshape(nr_data_test, time_data_amount*channel_width)\n",
    "\n",
    "            scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "\n",
    "            # scale everything using that scaler\n",
    "            data_train = scaler.transform(data_train)\n",
    "            data_test = scaler.transform(data_test)\n",
    "\n",
    "            #reshaping back to 2d features\n",
    "            data_train = data_train.reshape(nr_data_train, time_data_amount, channel_width)\n",
    "            data_test = data_test.reshape(nr_data_test, time_data_amount, channel_width)\n",
    "\n",
    "            labels_test = labels_test.reshape(nr_data_test,channel_width)\n",
    "            labels_train = labels_train.reshape(nr_data_train,channel_width)\n",
    "            labels_test = to_categorical(labels_test)\n",
    "            labels_train = to_categorical(labels_train)\n",
    "\n",
    "            class_weights = [0.75, 0.75, 0.9]\n",
    "            class_weights = np.array([class_weights[i] for i in range(len(class_weights))])\n",
    "\n",
    "            signal_size = time_data_amount\n",
    "\n",
    "            y = layers.Input(shape=(signal_size,channel_width), dtype='float32', name='Input')\n",
    "            x = layers.Reshape((1,time_data_amount,channel_width), input_shape=(time_data_amount,channel_width))(y)\n",
    "\n",
    "            x = layers.Conv2D(sf, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "            x = layers.Conv2D(sf, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "            if channel_width == 3:\n",
    "                x = layers.MaxPool2D(pool_size=[2,1], strides=[2,1], padding='valid', data_format='channels_first')(x)\n",
    "            else:\n",
    "                x = layers.MaxPool2D(pool_size=2, strides=2, padding='valid', data_format='channels_first')(x)\n",
    "            x = layers.Conv2D(sf*2, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "            x = layers.Conv2D(sf*2, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "            if channel_width < 8:\n",
    "                x = layers.MaxPool2D(pool_size=[2,1], strides=[2,1], padding='valid', data_format='channels_first')(x)\n",
    "            else:\n",
    "                x = layers.MaxPool2D(pool_size=[2, 2], strides=[2,2], padding='valid', data_format='channels_first')(x)        \n",
    "            x = layers.Conv2D(sf*4, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "            x = layers.Conv2D(sf*4, 3, padding='same', activation='relu', use_bias=True,data_format='channels_first')(x)\n",
    "            x = layers.Flatten()(x)\n",
    "\n",
    "\n",
    "            class_layer = [{}]*channel_width\n",
    "            output_layer = [{}]*channel_width\n",
    "            for iter in range(channel_width):\n",
    "                class_layer[iter] = layers.Dropout(rate=0.5)(x)\n",
    "                class_layer[iter] = layers.Dense(sf,activation='relu')(class_layer[iter])\n",
    "                class_layer[iter] = layers.Dropout(rate=0.2)(class_layer[iter])\n",
    "                output_layer[iter] = layers.Dense(nr_classes, activation='softmax', name=('out'+str(iter)))(class_layer[iter])\n",
    "\n",
    "\n",
    "\n",
    "            model = Model(inputs=[y], outputs=[out_layer for out_layer in output_layer])\n",
    "\n",
    "\n",
    "            isExist = os.path.exists(save_folder)\n",
    "            if not isExist:\n",
    "                os.makedirs(save_folder)\n",
    "                print('Created \"' + save_folder + '\" directory')\n",
    "            else:\n",
    "                print('\"'+ save_folder + '\" directory already existed - skipping')\n",
    "                continue\n",
    "\n",
    "\n",
    "            nr_params = count_params(model.trainable_weights)\n",
    "\n",
    "            cre = open(save_folder + '/' + 'Params_' + str(nr_params), 'x')\n",
    "\n",
    "\n",
    "            # ------------- model compilation --------------\n",
    "            ourAdam = Adam()\n",
    "            optimizer = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "            if using_weights == True:\n",
    "                    loss_func = weighted_mean_squared_error(class_weights)\n",
    "            else:\n",
    "                    loss_func = 'categorical_crossentropy'\n",
    "\n",
    "            metric_T = 'accuracy'\n",
    "            loss_dict = {}\n",
    "            metric_dict = {}\n",
    "            for iter in range(channel_width):\n",
    "                    loss_dict['out'+str(iter)] = loss_func\n",
    "                    metric_dict['out'+str(iter)] = metric_T\n",
    "\n",
    "\n",
    "\n",
    "            model.compile(optimizer=ourAdam, loss = loss_dict,\n",
    "                          metrics=metric_T) \n",
    "\n",
    "\n",
    "            if nr_params > 1000000:\n",
    "                ting = 1\n",
    "            elif nr_params > 500000:\n",
    "                ting = 2\n",
    "            elif nr_params > 250000:\n",
    "                ting = 4    \n",
    "            elif nr_params > 125000:\n",
    "                ting = 8\n",
    "            elif nr_params < 50000:\n",
    "                ting = 32\n",
    "            else:\n",
    "                ting = 16\n",
    "\n",
    "\n",
    "            BATCH_SIZE = 128*ting\n",
    "            EPOCH = 300 + 75 * ting\n",
    "\n",
    "            # Set the model training parameters\n",
    "            # Stop model training when the training loss is not dropped\n",
    "            callbacks_list = [callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss', \n",
    "                                    patience=math.floor(15 + ting * 1.3), \n",
    "                                    verbose=0, \n",
    "                                    mode='auto',\n",
    "                                    restore_best_weights=True,\n",
    "                                )\n",
    "                                        ]\n",
    "\n",
    "            # ------------- Starting model Training --------------\n",
    "\n",
    "            hist = model.fit(data_train,[labels_train[:,iter,:] for iter in range(channel_width)],\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = EPOCH, \n",
    "                      callbacks= callbacks_list,\n",
    "                      verbose=0,\n",
    "                      validation_split=0.25)\n",
    "\n",
    "\n",
    "            # Show loss curves\n",
    "            fig1 = plt.figure()\n",
    "            plt.title('Training loss')\n",
    "            plt.plot(hist.epoch, hist.history['loss'], label='train loss')\n",
    "            plt.plot(hist.epoch, hist.history['val_loss'], label='val_loss')\n",
    "            plt.legend()\n",
    "            plt.savefig(save_folder + '/%s Training loss.pdf' %(name), format='pdf')\n",
    "            #plt.show()\n",
    "            plt.close(fig1)\n",
    "\n",
    "            fig2 = plt.figure()\n",
    "            plt.title('Training accuracy')\n",
    "            plt.xlabel(\"Epoch #\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "\n",
    "            lossNames = ['out0_accuracy', 'out'+str(math.floor(channel_width/3))+'_accuracy', 'out'+str(math.floor(channel_width*2/3))+'_accuracy']\n",
    "            for (i, l) in enumerate(lossNames):\n",
    "                # plot the loss for both the training and validation data\n",
    "                title = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
    "                plt.plot(hist.epoch, hist.history[l], label=l)\n",
    "                plt.plot(hist.epoch, hist.history[\"val_\" + l],\n",
    "                    label=\"val_\" + l)\n",
    "                plt.legend()\n",
    "            plt.savefig(save_folder + '/%s Training acc.pdf' %(name), format='pdf')\n",
    "            #plt.show()\n",
    "            plt.close(fig2)\n",
    "\n",
    "            evalDict = model.evaluate(data_test,[labels_test[:,iter,:] for iter in range(channel_width)])\n",
    "            totalA = 0\n",
    "            for i in range(channel_width+1,channel_width+1+channel_width):\n",
    "                totalA += evalDict[i]\n",
    "\n",
    "            totalA /= channel_width\n",
    "            print(totalA)\n",
    "\n",
    "            # Saving dict of history and evaluation result\n",
    "            with open(save_folder + '/' + 'histDict', 'wb') as file_pi:\n",
    "                pickle.dump(hist.history, file_pi)\n",
    "\n",
    "            with open(save_folder + '/' + 'evalDict' + str(totalA), 'wb') as file_pi:\n",
    "                pickle.dump(evalDict, file_pi)\n",
    "\n",
    "            model.save(save_folder + '/' + name + '_Model')\n",
    "\n",
    "            #Test on test data\n",
    "            true_test_labels = np.argmax(labels_test, axis=-1)\n",
    "            test_predictions = model.predict(data_test)\n",
    "            test_result = np.argmax(test_predictions, axis=-1).T\n",
    "\n",
    "            #classification report\n",
    "            class_names = ['Empty channel', 'Wi-Fi', 'Bluetooth']\n",
    "            class_report = classification_report(true_test_labels.flatten(), test_result.flatten(),target_names=class_names)\n",
    "\n",
    "            with open(save_folder + '/' + 'classReportString', 'wb') as file_pi:\n",
    "                pickle.dump(class_report, file_pi)\n",
    "\n",
    "            #Confusion matric plot\n",
    "            plt.figure()\n",
    "            ConfusionMatrixDisplay.from_predictions(true_test_labels.flatten(), test_result.flatten(),normalize='true',cmap='Greens',colorbar=False,display_labels=class_names)\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.savefig(save_folder +'/confusion_matrix_'+ name +'.pdf', format='pdf')\n",
    "            plt.close()\n",
    "\n",
    "            class_reportDict = classification_report(true_test_labels.flatten(), test_result.flatten(),output_dict=True, target_names=class_names)\n",
    "            for key in class_reportDict:\n",
    "                try:\n",
    "                    class_reportDict[key]['Samples'] = class_reportDict[key].pop('support')\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            #print(class_reportDict)\n",
    "            class_reportDict.pop('accuracy')\n",
    "            df = pd.DataFrame(class_reportDict).transpose().round(decimals=3)\n",
    "            dfi.export(df, save_folder + '/' +name + \"_ClassReport.png\", table_conversion=\"matplotlib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdb41d0cc79015bdcf6a93996e5168bcca6d7c1b72f3e6d100b9698b1014ae19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
